from flask import Flask, request, jsonify
from flask_cors import CORS, cross_origin
from rag.rag import RAG
from reflection.reflection import Reflection
from langchain_openai import ChatOpenAI
from router import SemanticRouter
from langchain_community.tools.tavily_search import TavilySearchResults
import os
import openai
from dotenv import load_dotenv

load_dotenv()

os.environ["TAVILY_API_KEY"] = os.getenv("TAVILY_API_KEY")
tavily_search = TavilySearchResults(max_results=3)
mongodbUrl = os.getenv("MONGODB_URL")
dbName = os.getenv("DB_NAME")
dbCollection = os.getenv("DB_COLLECTION")
API_KEY = os.getenv("OPENAI_API_KEY")
llm = ChatOpenAI(api_key=API_KEY)
limitReturn = 5  ### Number of results return from semantic search.
modelEmbeddingName = os.getenv("MODEL_EMBEDDING")

rag = RAG(
        mongodbUrl = mongodbUrl,
        dbName = dbName,
        dbCollection = dbCollection,
        llm = llm,
        limitReturn = limitReturn,
        modelEmbeddingName = modelEmbeddingName
)

## semantic router
gpt = openai.OpenAI(api_key=API_KEY)
semanticRouter = SemanticRouter(llm=gpt)

### reflection
reflection = Reflection(llm=gpt)
# Flask Server Backend
app = Flask(__name__)

# Apply Flask CORS
CORS(app)
app.config['CORS_HEADERS'] = 'Content-Type'

@app.route('/add', methods=['POST'])
@cross_origin(origin='*')

def handle_query():
    data = list(request.get_json())
    query = data[-1]["content"][0]["text"]
    inital_query = rag.query_lower(query)

    if not inital_query:
        return jsonify({'error': 'No query provided'}), 400

    reflect_query = reflection(data)
    query = reflect_query
    print("inital_query>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>: ", inital_query)
    print("query>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>: ", query)
    data.append({
        "role": "user",
        "content": [
            {   
                "type": "text",
                "text": inital_query,
            }
        ]
    })

    guidedRoute = semanticRouter.guide(query)  # Truy v·∫•n semantic router


    if guidedRoute == "course":

        info_course_list = rag.enhance_prompt(query)

        if info_course_list: ### DB
            
            prompt = f""" B·∫°n h√£y tr·ªü th√†nh chuy√™n gia tu v·∫•n kh√≥a h·ªçc c·ªßa Tr∆∞·ªùng ƒê·∫°i H·ªçc C√¥ng Ngh·ªá Th√¥ng Tin, v·ªõi c√¢u h·ªèi nh∆∞ sau: \n "{inital_query}."\n
            V√† b·∫°n ƒë∆∞·ª£c cung c·∫•p m·ªôt s·ªë th√¥ng tin v·ªÅ kh√≥a h·ªçc nh∆∞ sau: "{info_course_list}"\n
            H√£y t∆∞ v·∫•n cho sinh vi√™n.
         
            L∆∞u √Ω, CH·ªà T∆Ø V·∫§N TR·ª∞C TI·∫æP, KH√îNG N√ìI NH·ªÆNG C√ÇU NH∆Ø "D·ª∞A V√ÄO TH√îNG TIN TUI C√ì".
            - Tr·ª±c tri·∫øp tr·∫£ l·ªùi th·∫≥ng v√†o v·∫•n ƒë·ªÅ.
            """

            response = rag.generate_content(prompt)
            model_response = response.content
            # print("model_response: >>>>>>>>>>>>>>>>>>>>>>", model_response)

        else:  ## Search
            result_tavily = tavily_search.run(query)
            # Ki·ªÉm tra k·∫øt qu·∫£ tr·∫£ v·ªÅ
            if not result_tavily or len(result_tavily) == 0:
                return jsonify({
                    'content': [
                        {
                            "type": "text",
                            'text': "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p tr√™n Internet.",
                        }
                    ],
                    'role': 'model'
                })

            # N·∫øu Tavily c√≥ d·ªØ li·ªáu, x·ª≠ l√Ω v√† hi·ªÉn th·ªã k·∫øt qu·∫£
            model_response = "\n\n".join([
                f"üîé **{item['title']}**\n{item['url']}\nüìå {item['content']}"
                for item in result_tavily
            ])
            return jsonify({
                    'content': [
                        {
                            "type": "text",
                            'text': model_response,
                        }
                    ],
                    'role': 'model'
                })

    else:  ### "LLMs"

        chatchit_query = f"""
                B·∫°n l√† tr·ª£ l√Ω ·∫£o c·ªßa Tr∆∞·ªùng ƒê·∫°i h·ªçc C√¥ng ngh·ªá Th√¥ng tin, t√™n l√† ƒê·∫≠u ƒê·∫≠u. 
                Vai tr√≤ c·ªßa b·∫°n l√† h·ªó tr·ª£ v√† tr√≤ chuy·ªán th√¢n thi·ªán v·ªõi sinh vi√™n trong m·ªçi t√¨nh hu·ªëng h·ªçc t·∫≠p, h√†nh ch√≠nh ho·∫∑c ƒë·ªùi s·ªëng sinh vi√™n. 
                H√£y ph·∫£n h·ªìi d·ª±a tr√™n n·ªôi dung sau t·ª´ sinh vi√™n:

                "{inital_query}"
                """
        print("chatchit_query: >>>>>>>>>>>>>>>>>>>>>>>>>>>", chatchit_query)
        response = llm.invoke(chatchit_query)
        model_response = response.content
        
    return jsonify({
        'content': [
            {
                "type": "text",
                'text': model_response,
            }
        ],
        'role': 'model'
        })

@app.post("/reset")
def reset_session():
    reflection.clear_histories()
    print("CLEAR HISTORIES")
    return {"message": "ƒê√£ reset"}


if __name__ == '__main__':
    app.run(host='0.0.0.0', port='6868') 